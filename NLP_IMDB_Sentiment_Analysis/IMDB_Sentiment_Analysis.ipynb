{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jianhenghou/anaconda2/envs/py36/lib/python3.6/site-packages/lightgbm/__init__.py:46: UserWarning: Starting from version 2.2.1, the library file in distribution wheels for macOS is built by the Apple Clang (Xcode_8.3.3) compiler.\n",
      "This means that in case of installing LightGBM from PyPI via the ``pip install lightgbm`` command, you don't need to install the gcc compiler anymore.\n",
      "Instead of that, you need to install the OpenMP library, which is required for running LightGBM on the system with the Apple Clang compiler.\n",
      "You can install the OpenMP library by the following command: ``brew install libomp``.\n",
      "  \"You can install the OpenMP library by the following command: ``brew install libomp``.\", UserWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "import re, unicodedata\n",
    "import os\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tokenizer import tokenize\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)\n",
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files(path):\n",
    "    train_pos_files = os.listdir(path)\n",
    "    raw_text = []\n",
    "    for filename in train_pos_files:\n",
    "        fb = open(path+filename,'r')\n",
    "        raw_text.append(fb.readline())\n",
    "        fb.close()  \n",
    "    return raw_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"havent\":\"have not\",\"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "def normalize(words):\n",
    "    word_list = []\n",
    "    for word in words:\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "        new_word = new_word.lower()\n",
    "        \"\"\"Remove punctuation except for \"?\" and \"!\" from list of tokenized words\"\"\"\n",
    "        if contraction_mapping.__contains__(new_word):\n",
    "            new_word = contraction_mapping[new_word]\n",
    "        new_word = re.sub(r'[^\\w\\s!?]', '', new_word)\n",
    "        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        new_word = lemmatizer.lemmatize(new_word, pos='v')\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        if new_word.isdigit():\n",
    "            new_word = 'digit'\n",
    "        if new_word != '':\n",
    "            word_list.append(new_word)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read train data and test data from files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read all train data\n",
    "train_pos = read_files('./aclImdb/train/pos/')\n",
    "train_neg = read_files('./aclImdb/train/neg/')\n",
    "raw_train_text = train_pos + train_neg\n",
    "train_label = [1] * len(train_pos) + [0] * len(train_neg)\n",
    "raw_train_text, train_label = shuffle(raw_train_text, train_label, random_state=0)\n",
    "\n",
    "# read all test data\n",
    "test_pos = read_files('./aclImdb/test/pos/')\n",
    "test_neg = read_files('./aclImdb/test/neg/')\n",
    "raw_test_text = test_pos + test_neg\n",
    "test_label = [1] * len(test_pos) + [0] * len(test_neg)\n",
    "raw_test_text, test_label = shuffle(raw_test_text, test_label, random_state=0)\n",
    "\n",
    "raw_corpus_text = raw_train_text + raw_test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing for train text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_text = []\n",
    "for each in raw_train_text:\n",
    "    tmp = each.replace('-', ' ').replace('<br />',\"\")\n",
    "    train_text.append(\"\".join(word + ' ' for word in normalize([word.txt for word in tokenize(tmp) if word.txt != None])).strip())\n",
    "test_text = []\n",
    "for each in raw_test_text:\n",
    "    tmp = each.replace('-', ' ').replace('<br />',\"\")\n",
    "    test_text.append(\"\".join(word + ' ' for word in normalize([word.txt for word in tokenize(tmp) if word.txt != None])).strip())\n",
    "processed_corpus_text = train_text + test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write corpus into a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('IMDB_raw_corpus_text.txt','w') as c:\n",
    "#     for line in raw_corpus_text:\n",
    "#         c.write(line + '\\n')\n",
    "#     c.close()\n",
    "# with open('IMDB_raw_train_text.txt','w') as tr:\n",
    "#     for line in train_text:\n",
    "#         tr.write(line + '\\n')\n",
    "#     tr.close()\n",
    "# with open('IMDB_raw_test_text.txt','w') as te:\n",
    "#     for line in test_text:\n",
    "#         te.write(line + '\\n')\n",
    "#     te.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_analysis(words):\n",
    "    contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"havent\":\"have not\",\"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "    stop_words = set([\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"])\n",
    "\n",
    "    word_list = []\n",
    "    for word in words:\n",
    "        \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "        new_word = new_word.lower()\n",
    "        \"\"\"Remove punctuation except for \"?\" and \"!\" from list of tokenized words\"\"\"\n",
    "        if contraction_mapping.__contains__(new_word):\n",
    "            new_word = contraction_mapping[new_word]\n",
    "        new_word = re.sub(r'[^\\w\\s!?]', '', new_word)\n",
    "        \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        new_word = lemmatizer.lemmatize(new_word, pos='v')\n",
    "        \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "        if new_word.isdigit():\n",
    "            new_word = 'digit'\n",
    "        if new_word in stop_words:\n",
    "            new_word = ''\n",
    "        if new_word != '':\n",
    "            word_list.append(new_word)\n",
    "    return word_list\n",
    "\n",
    "text_analysis = []\n",
    "for each in raw_corpus_text:\n",
    "    tmp = each.replace('-', ' ').replace('<br />',\"\")\n",
    "    text_analysis.append(\"\".join(word + ' ' for word in normalize_analysis([word.txt for word in tokenize(tmp) if word.txt != None])).strip())\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import collections\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "sns.set(font_scale=1.3)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "bow = cv.fit_transform(text_analysis)\n",
    "word_freq = dict(zip(cv.get_feature_names(), np.asarray(bow.sum(axis=0)).ravel()))\n",
    "word_counter = collections.Counter(word_freq)\n",
    "word_counter_df = pd.DataFrame(word_counter.most_common(20), columns = ['word', 'freq'])\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "sns.barplot(x=\"word\", y=\"freq\", data=word_counter_df, palette=\"PuBuGn_d\", ax=ax)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the train text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_text</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>First off, I am a huge fan of Tolkien, and as one I will base most of my critic on his books.&lt;br /&gt;&lt;br /&gt;The movie is a standard adventure movie, well made with nifty special effects, nice sound track and fine acting. Now if this movie was called something else than lord of the rings the reviews wouldn't be half this good as they are here.&lt;br /&gt;&lt;br /&gt;The problem of the movie is that it takes the basic story line from Tolkiens books but then it goes and \"hollywoods\" everything it can, numerous scenes from the book are eighter missing or changed quite a lot, the characters are changed from the book also, a thing that I think should be punishable ! What the movie lacks is deep insight of the characters in it, I know that it is almost impossible to make a good film out of a good book, and it didn't work here eighter, mostly the motivation of the characters is left hazy at best.&lt;br /&gt;&lt;br /&gt;As a adventure movie it would rate 7+ / 10 As a adaptation of Tolkien it rates 2 / 10&lt;br /&gt;&lt;br /&gt;I mean honestly, what on earth was Arwen doing at rivendell ford ? And as for the comments that this movie \"is the best ever\" I can only say that eighter you are very young, or you havent seen good movies...&lt;br /&gt;&lt;br /&gt;Peter Jackson should have called this movie an adventure movie based on the lord of the rings.</td>\n",
       "      <td>first off i be a huge fan of tolkien and as one i will base most of my critic on his book the movie be a standard adventure movie well make with nifty special effect nice sound track and fine act now if this movie be call something else than lord of the ring the review would not be half this good as they be here the problem of the movie be that it take the basic story line from tolkiens book but then it go and hollywoods everything it can numerous scenes from the book be eighter miss or change quite a lot the character be change from the book also a thing that i think should be punishable ! what the movie lack be deep insight of the character in it i know that it be almost impossible to make a good film out of a good book and it did not work here eighter mostly the motivation of the character be leave hazy at best as a adventure movie it would rate digit digit as a adaptation of tolkien it rat digit 10i mean honestly what on earth be arwen do at rivendell ford ? and as for the comment that this movie be the best ever i can only say that eighter you be very young or you have not see good movies peter jackson should have call this movie an adventure movie base on the lord of the ring</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         raw_text  \\\n",
       "173  First off, I am a huge fan of Tolkien, and as one I will base most of my critic on his books.<br /><br />The movie is a standard adventure movie, well made with nifty special effects, nice sound track and fine acting. Now if this movie was called something else than lord of the rings the reviews wouldn't be half this good as they are here.<br /><br />The problem of the movie is that it takes the basic story line from Tolkiens books but then it goes and \"hollywoods\" everything it can, numerous scenes from the book are eighter missing or changed quite a lot, the characters are changed from the book also, a thing that I think should be punishable ! What the movie lacks is deep insight of the characters in it, I know that it is almost impossible to make a good film out of a good book, and it didn't work here eighter, mostly the motivation of the characters is left hazy at best.<br /><br />As a adventure movie it would rate 7+ / 10 As a adaptation of Tolkien it rates 2 / 10<br /><br />I mean honestly, what on earth was Arwen doing at rivendell ford ? And as for the comments that this movie \"is the best ever\" I can only say that eighter you are very young, or you havent seen good movies...<br /><br />Peter Jackson should have called this movie an adventure movie based on the lord of the rings.   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 text  \\\n",
       "173  first off i be a huge fan of tolkien and as one i will base most of my critic on his book the movie be a standard adventure movie well make with nifty special effect nice sound track and fine act now if this movie be call something else than lord of the ring the review would not be half this good as they be here the problem of the movie be that it take the basic story line from tolkiens book but then it go and hollywoods everything it can numerous scenes from the book be eighter miss or change quite a lot the character be change from the book also a thing that i think should be punishable ! what the movie lack be deep insight of the character in it i know that it be almost impossible to make a good film out of a good book and it did not work here eighter mostly the motivation of the character be leave hazy at best as a adventure movie it would rate digit digit as a adaptation of tolkien it rat digit 10i mean honestly what on earth be arwen do at rivendell ford ? and as for the comment that this movie be the best ever i can only say that eighter you be very young or you have not see good movies peter jackson should have call this movie an adventure movie base on the lord of the ring   \n",
       "\n",
       "     sentiment  \n",
       "173  0          "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame(list(zip(raw_train_text,train_text, train_label)), columns =['raw_text','text', 'sentiment'])\n",
    "test_df = pd.DataFrame(list(zip(raw_test_text,test_text, test_label)), columns =['raw_text','text', 'sentiment'])\n",
    "# pd.set_option('display.max_colwidth', 500)\n",
    "test_df.loc[[173]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train TF-IDF embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=10000, min_df=2,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=['he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'a', 'an', 'the', 'and', 'of', 'at', 'by', 'for', 'with', 'about', 'into', 'through', 'during', 'to', 'from', 'then', 'once', 'here', 'there', 'both', 'each', 'some', 'such', 'own', 'than', 'don', 'now'],\n",
       "        strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words_list = [\"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"a\", \"an\", \"the\", \"and\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"into\", \"through\", \"during\", \"to\", \"from\", \"then\", \"once\", \"here\", \"there\", \"both\", \"each\",  \"some\", \"such\",\"own\", \"than\", \"don\", \"now\"]\n",
    "Tfidf_vectorizer = TfidfVectorizer(max_features=10000,\n",
    "                                   min_df=2, \n",
    "                                   ngram_range=(1, 3),\n",
    "                                   analyzer='word',\n",
    "                                   stop_words = stop_words_list)\n",
    "\n",
    "Tfidf_vectorizer.fit(processed_corpus_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the train/test text into the corresponding TF-IDF embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = Tfidf_vectorizer.transform(train_text)\n",
    "test = Tfidf_vectorizer.transform(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_features(text_list):\n",
    "    # length feature     \n",
    "    feature_length = []\n",
    "    # 'i' count feature\n",
    "    feature_i_count = []\n",
    "    for line in text_list:\n",
    "        cnt = Counter()\n",
    "        tokenzied = [word.txt for word in tokenize(line) if word.txt != None]\n",
    "        for word in tokenzied:\n",
    "            if word == 'i':\n",
    "                cnt['i'] += 1\n",
    "        feature_i_count.append(cnt['i'])\n",
    "        feature_length.append(len(tokenzied))\n",
    "    return feature_length, feature_i_count\n",
    "\n",
    "def add_pos_neg_weights(text_list, label):\n",
    "    # word pos/neg weight   \n",
    "    conf = SparkConf() \\\n",
    "            .setAppName(\"add_pos_neg_weight\") \\\n",
    "            .set(\"spark.driver.host\", \"localhost\")\n",
    "    sc = SparkContext(conf=conf)\n",
    "    text_RDD = sc.parallelize(list(zip(text_list, label)))\n",
    "    \n",
    "    pos_dict = text_RDD.filter(lambda x: x[1] == 1)\\\n",
    "                        .flatMap(lambda x: x[0].split(\" \"))\\\n",
    "                                .map(lambda x: (x, 1))\\\n",
    "                                    .reduceByKey(lambda a, b: a + b)\\\n",
    "                                            .collectAsMap()\n",
    "    neg_dict = text_RDD.filter(lambda x: x[1] == 0)\\\n",
    "                            .flatMap(lambda x: x[0].split(\" \"))\\\n",
    "                                    .map(lambda x: (x, 1))\\\n",
    "                                            .reduceByKey(lambda a, b: a + b)\\\n",
    "                                                    .collectAsMap()\n",
    "#     whole_RDD = pos_RDD.union(neg_RDD).reduceByKey(lambda x,y : x+y).map(lambda x: (x[0], (x[1] + 2)**2))    \n",
    "#     word_sentiment_weight = whole_RDD.collectAsMap()\n",
    "    sc.stop()\n",
    "    word_sentiment_weight = {}\n",
    "    all_words = set(pos_dict.keys()).union(set(neg_dict.keys()))\n",
    "    for key in all_words:\n",
    "        if key is not pos_dict:\n",
    "            pos_dict[key] = 0\n",
    "        if key is not neg_dict:\n",
    "            neg_dict[key] = 0\n",
    "        N = pos_dict[key] + neg_dict[key]\n",
    "        if key == 'ponyo':\n",
    "            print('yes')\n",
    "        word_sentiment_weight[key] = float((N + 2)**2) / ((pos_dict[key] + 1) * (neg_dict[key] + 1))\n",
    "\n",
    "    return word_sentiment_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # new features: feat_len, feat_i\n",
    "# feat_len_train, feat_i_train = add_features(train_text)\n",
    "# feat_len_test, feat_i_test = add_features(test_text)\n",
    "\n",
    "# # sentiment weights for each word\n",
    "# from pyspark import SparkContext, SparkConf\n",
    "# feat_sentiment_weight = add_pos_neg_weights(train_text, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# feat_sentiment_weight_list = []\n",
    "# for feature_name in Tfidf_vectorizer.get_feature_names():\n",
    "#     if feature_name not in feat_sentiment_weight:\n",
    "#         feat_sentiment_weight_list.append(1)\n",
    "#     else:\n",
    "#         feat_sentiment_weight_list.append(feat_sentiment_weight[feature_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # adding new features if neccessary\n",
    "# boosted_train = train.toarray() * np.array(feat_sentiment_weight_list)\n",
    "# boosted_test = test.toarray() * np.array(feat_sentiment_weight_list)\n",
    "# boosted_train = np.c_[boosted_train, feat_len_train, feat_i_train] \n",
    "# boosted_test = np.c_[boosted_test, feat_len_test, feat_i_test] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.57      0.66     12500\n",
      "           1       0.66      0.84      0.74     12500\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     25000\n",
      "   macro avg       0.72      0.70      0.70     25000\n",
      "weighted avg       0.72      0.70      0.70     25000\n",
      "\n",
      "Accuracy: 0.7045600000000001\n"
     ]
    }
   ],
   "source": [
    "# Analysis using NLTK Vader SentimentAnalyser\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "for train\n",
    "sia_y_pred_train = []\n",
    "for sentence in train_text:\n",
    "        ss = sia.polarity_scores(sentence)\n",
    "        if ss['compound'] >= 0.4:\n",
    "            sia_y_pred_train.append(1)\n",
    "        elif ss['compound'] <= -0.4:\n",
    "            sia_y_pred_train.append(-1)\n",
    "        else:\n",
    "            sia_y_pred_train.append(0)\n",
    "\n",
    "print(classification_report(test_label, sia_y_pred_train))\n",
    "print(\"Accuracy:\", balanced_accuracy_score(test_label, sia_y_pred_train))\n",
    "\n",
    "#  for test\n",
    "sia_y_pred_test = []\n",
    "value = []\n",
    "for sentence in test_text:\n",
    "        ss = sia.polarity_scores(sentence)\n",
    "        value.append(ss['compound'])\n",
    "        if ss['compound'] > 0.3:\n",
    "            sia_y_pred_test.append(1)\n",
    "        else:\n",
    "            sia_y_pred_test.append(0)\n",
    "print(classification_report(test_label, sia_y_pred_test))\n",
    "print(\"Accuracy:\", balanced_accuracy_score(test_label, sia_y_pred_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logist Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l2'}\n",
      "0.8949228944646784\n"
     ]
    }
   ],
   "source": [
    "#  best {'C': 1, 'penalty': 'l2'} for train\n",
    "#  best {'C': 0.1, 'penalty': 'l2'} for boosted_train\n",
    "param_grid = dict(C=[0.01, 0.1, 1], penalty =['l1','l2'])\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), \n",
    "                    param_grid=param_grid, \n",
    "                    cv=5, \n",
    "                    scoring=['f1', 'roc_auc', 'balanced_accuracy'], \n",
    "                    refit='f1',\n",
    "                    n_jobs=-1)\n",
    "grid.fit(train, train_label)\n",
    "LR_model = grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Model Evalucation=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90     12500\n",
      "           1       0.89      0.90      0.90     12500\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     25000\n",
      "   macro avg       0.90      0.90      0.90     25000\n",
      "weighted avg       0.90      0.90      0.90     25000\n",
      "\n",
      "============Test Accuracy===========\n",
      "0.8957200000000001\n"
     ]
    }
   ],
   "source": [
    "LR_y_pred = LR_model.predict(test)\n",
    "print(\"==========Model Evalucation=========\")\n",
    "print(classification_report(test_label, LR_y_pred))\n",
    "print(\"============Test Accuracy===========\")\n",
    "print(balanced_accuracy_score(test_label, LR_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'C': 4, 'gamma': 1, 'kernel': 'rbf'} forr 10000, 1\n",
    "# {'C': 4, 'gamma': 1, 'kernel': 'rbf'} for 30000, 1-5\n",
    "param_grid = dict(gamma=[0.1, 1], C=[1, 2, 4, 10], kernel=['rbf'])\n",
    "grid = GridSearchCV(SVC(), \n",
    "                    param_grid=param_grid, \n",
    "                    scoring=['f1'], \n",
    "                    refit='f1',\n",
    "                    n_jobs=-1)\n",
    "grid.fit(train, train_label)\n",
    "SVM_model = grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_y_pred = SVM_model.predict(test)\n",
    "print(\"==========Model Evalucation=========\")\n",
    "print(classification_report(test_label, SVM_y_pred))\n",
    "print(\"============Test Accuracy===========\")\n",
    "print(balanced_accuracy_score(test_label, SVM_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 4, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.9001096993221857\n"
     ]
    }
   ],
   "source": [
    "# {'C': 4, 'gamma': 1, 'kernel': 'rbf'} forr 10000, 1\n",
    "# {'C': 4, 'gamma': 1, 'kernel': 'rbf'} for 30000, 1-5\n",
    "param_grid = dict(gamma=[1], C=[1, 2, 4], kernel=['rbf'])\n",
    "grid = GridSearchCV(SVC(), \n",
    "                    param_grid=param_grid, \n",
    "                    scoring=['f1'], \n",
    "                    refit='f1',\n",
    "                    n_jobs=-1)\n",
    "grid.fit(train, train_label)\n",
    "SVM_model = grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Model Evalucation=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90     12500\n",
      "           1       0.90      0.90      0.90     12500\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     25000\n",
      "   macro avg       0.90      0.90      0.90     25000\n",
      "weighted avg       0.90      0.90      0.90     25000\n",
      "\n",
      "============Test Accuracy===========\n",
      "0.90036\n"
     ]
    }
   ],
   "source": [
    "SVM_y_pred = SVM_model.predict(test)\n",
    "print(\"==========Model Evalucation=========\")\n",
    "print(classification_report(test_label, SVM_y_pred))\n",
    "print(\"============Test Accuracy===========\")\n",
    "print(balanced_accuracy_score(test_label, SVM_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 40, 'n_estimators': 200, 'oob_score': True}\n",
      "0.858634200193079\n"
     ]
    }
   ],
   "source": [
    "#  best {'criterion': 'entropy', 'max_depth': 40, 'n_estimators': 200, 'oob_score': True}\n",
    "param_grid = dict(n_estimators=[50, 100, 200], criterion=[\"entropy\"], oob_score=[True],max_depth=[20, 40, 60, 80,100])\n",
    "\n",
    "grid = GridSearchCV(RandomForestClassifier(), \n",
    "                    param_grid=param_grid, \n",
    "                    scoring=['f1'], \n",
    "                    refit='f1',\n",
    "                    n_jobs=-1)\n",
    "grid.fit(train, train_label)\n",
    "RF_model = grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  30000 1-5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Model Evalucation=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.84      0.85     12500\n",
      "           1       0.84      0.88      0.86     12500\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n",
      "============Test Accuracy===========\n",
      "0.85724\n"
     ]
    }
   ],
   "source": [
    "RF_y_pred = RF_model.predict(test)\n",
    "print(\"==========Model Evalucation=========\")\n",
    "print(classification_report(test_label, RF_y_pred))\n",
    "print(\"============Test Accuracy===========\")\n",
    "print(balanced_accuracy_score(test_label, RF_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(train.toarray(), columns=Tfidf_vectorizer.get_feature_names())\n",
    "y_train = pd.DataFrame(train_label)\n",
    "_ = plot_feature_importances(RF_model, X_train, y_train, top_n=train.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 3000}\n",
      "0.8788980982857951\n"
     ]
    }
   ],
   "source": [
    "# 2000 best\n",
    "#  {'n_estimators': 3000} for 30000, 1-5 gram\n",
    "cv_params = {'n_estimators': [2000, 3000, 4000]}\n",
    "other_params = {'learning_rate': 0.1, 'n_estimators': 500, 'max_depth': 5, 'min_child_weight': 1, 'seed': 0,\n",
    "                    'subsample': 0.8, 'colsample_bytree': 0.8, 'gamma': 0, 'reg_alpha': 0, 'reg_lambda': 1,\n",
    "                'early_stopping_rounds': 20}\n",
    "xgb_model = xgb.XGBClassifier(**other_params)\n",
    "grid = GridSearchCV(xgb_model, \n",
    "                    param_grid=cv_params, \n",
    "                    scoring=['f1'], \n",
    "                    refit='f1',\n",
    "                    n_jobs=-1)\n",
    "grid.fit(train, train_label)\n",
    "XGBoost_model = grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Model Evalucation=========\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.88      0.88     12500\n",
      "           1       0.88      0.89      0.89     12500\n",
      "\n",
      "   micro avg       0.89      0.89      0.89     25000\n",
      "   macro avg       0.89      0.89      0.89     25000\n",
      "weighted avg       0.89      0.89      0.89     25000\n",
      "\n",
      "============Test Accuracy===========\n",
      "0.88556\n"
     ]
    }
   ],
   "source": [
    "xgb_y_pred = XGBoost_model.predict(test)\n",
    "print(\"==========Model Evalucation=========\")\n",
    "print(classification_report(test_label, xgb_y_pred))\n",
    "print(\"============Test Accuracy===========\")\n",
    "print(balanced_accuracy_score(test_label, xgb_y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembled model (SVM + LR + SIA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_text</th>\n",
       "      <th>true_label</th>\n",
       "      <th>LR_predict</th>\n",
       "      <th>SVM_predict</th>\n",
       "      <th>SIA_predict</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>like many western pennsylvania history buff i have be really look forward to this much herald pbs program that be produce by pittsburghs wqed however i must say now that i be somewhat disappoint on the positive side i believe that overall this film do a fair job of explain the main issue and describe the events of the so call french and indian war in particular its presentation of the indians point of view be somewhat new and quite interest although it certainly be at time over emphasize also on the positive side the blend of narrative and action scenes be well do and come across somewhat better than many of these typical documentaries make up of experts interview and picture still a la ken burn on the negative side many of the battle do have a somewhat stag look and many important aspects of the war be overlook most of all i be very disappoint and frustrate by how little importance be give to forbess successful campaign of digit against fort duquesne as compare to the earlier failures of digit by washington and digit by braddock in particular i be somewhat incredulous that there be no mention of colonel henry bouquet the swiss mercenary in the british service who be most responsible for forbes success finally i could not believe the complete omission of the digit battle of bushy run that start as a re run of braddocks defeat but end up as the victory that decide the outcome of pontiacs war thank to the wiles of the same colonel bouquet who certainly must rank as one of the most successful british commanders of this war</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>there is a brand new killer on the loose and he is do gods work yeah right ! this killer make jason voorhes look like a chump and freddy krueger look like a rag doll against this dude he be jacob goodnight wwes glen kane jacobs a digit monster who wield a axe and a hook and chain those weapons be nothing to him his real finisher be rip out eyeball from the victims sockets that be totally methodical ! when the encounter happen digit years earlier jacob kill a rookie cop and maim the veteran after put a bullet in his head how on earth do goodnight survive after digit years ? now he is in the condemn hotel call blackwell and this hotel get a lot of stories to tell i think this movie be haunt as well as interest i like the part where goodnight check out one of the girls tattoo on her back and goodnight himself be really derange thank to his maniacal mother if you think friday the digit th be something you better think again this movie will leave you on the edge of you seat and i think the eyeball rip be bone chill this movie prove it point and it was not a waste of my time i enjoy it the title do not lie ! rat digit out of digit star !</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>as if the world had not already get enough cheap jaw imitations writer boaz davidson decide to make the sequel to his ropey but reasonably enjoyable creature feature octopus a complete rip off of spielbergs classic right down to have a concern cop who no one believe and a mayor more worry about his digit th july celebrations than people live even in the hand of an extremely skilled director it be unlikely that this derivative rubbish could have be anything other than hokey b movie garbage but with yossi wein yes the yossi wein ! call the shots behind the camera a man with a fraction of mrspielbergs talent i estimate about digit digit th octopus digit be guarantee to be every bite as bad as one might imagine ! the predictable and extremely cliched plot is not worth describe in much detail substitute jaw amity island with new york and bruce the shark with a giant rubber octopus and you will get the gist although several point about the film be definitely worth mention simply because they be so funny all of the octopus attack involve the actors struggle to make incredibly fake look giant tentacles look real which be hilarious to behold bulgarias capital sofia unconvincingly stand in for new york and overuse of stock footage make the illusion even less convince best of all a silly dream sequence that see the rubber octopus attack our hero atop the statue of liberty be not only gut bustingly stupid but also feature some truly dreadful special effect davidsons script also does not know when to quit there be several point in this film at which it could have and probably should have end but the action go on and on with the octopus survive several explosions and cause a tunnel to collapse trap the film love interest and a bunch of kid before finally be blow to smithereens by the hero sometimes very silly always awful technically but never actually scary this stv stinker may find fan amongst those who actively seek out cinematic trash most normal people however would be advise to steer well clear</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               test_text  \\\n",
       "0  like many western pennsylvania history buff i have be really look forward to this much herald pbs program that be produce by pittsburghs wqed however i must say now that i be somewhat disappoint on the positive side i believe that overall this film do a fair job of explain the main issue and describe the events of the so call french and indian war in particular its presentation of the indians point of view be somewhat new and quite interest although it certainly be at time over emphasize also on the positive side the blend of narrative and action scenes be well do and come across somewhat better than many of these typical documentaries make up of experts interview and picture still a la ken burn on the negative side many of the battle do have a somewhat stag look and many important aspects of the war be overlook most of all i be very disappoint and frustrate by how little importance be give to forbess successful campaign of digit against fort duquesne as compare to the earlier failures of digit by washington and digit by braddock in particular i be somewhat incredulous that there be no mention of colonel henry bouquet the swiss mercenary in the british service who be most responsible for forbes success finally i could not believe the complete omission of the digit battle of bushy run that start as a re run of braddocks defeat but end up as the victory that decide the outcome of pontiacs war thank to the wiles of the same colonel bouquet who certainly must rank as one of the most successful british commanders of this war                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "1  there is a brand new killer on the loose and he is do gods work yeah right ! this killer make jason voorhes look like a chump and freddy krueger look like a rag doll against this dude he be jacob goodnight wwes glen kane jacobs a digit monster who wield a axe and a hook and chain those weapons be nothing to him his real finisher be rip out eyeball from the victims sockets that be totally methodical ! when the encounter happen digit years earlier jacob kill a rookie cop and maim the veteran after put a bullet in his head how on earth do goodnight survive after digit years ? now he is in the condemn hotel call blackwell and this hotel get a lot of stories to tell i think this movie be haunt as well as interest i like the part where goodnight check out one of the girls tattoo on her back and goodnight himself be really derange thank to his maniacal mother if you think friday the digit th be something you better think again this movie will leave you on the edge of you seat and i think the eyeball rip be bone chill this movie prove it point and it was not a waste of my time i enjoy it the title do not lie ! rat digit out of digit star !                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "2  as if the world had not already get enough cheap jaw imitations writer boaz davidson decide to make the sequel to his ropey but reasonably enjoyable creature feature octopus a complete rip off of spielbergs classic right down to have a concern cop who no one believe and a mayor more worry about his digit th july celebrations than people live even in the hand of an extremely skilled director it be unlikely that this derivative rubbish could have be anything other than hokey b movie garbage but with yossi wein yes the yossi wein ! call the shots behind the camera a man with a fraction of mrspielbergs talent i estimate about digit digit th octopus digit be guarantee to be every bite as bad as one might imagine ! the predictable and extremely cliched plot is not worth describe in much detail substitute jaw amity island with new york and bruce the shark with a giant rubber octopus and you will get the gist although several point about the film be definitely worth mention simply because they be so funny all of the octopus attack involve the actors struggle to make incredibly fake look giant tentacles look real which be hilarious to behold bulgarias capital sofia unconvincingly stand in for new york and overuse of stock footage make the illusion even less convince best of all a silly dream sequence that see the rubber octopus attack our hero atop the statue of liberty be not only gut bustingly stupid but also feature some truly dreadful special effect davidsons script also does not know when to quit there be several point in this film at which it could have and probably should have end but the action go on and on with the octopus survive several explosions and cause a tunnel to collapse trap the film love interest and a bunch of kid before finally be blow to smithereens by the hero sometimes very silly always awful technically but never actually scary this stv stinker may find fan amongst those who actively seek out cinematic trash most normal people however would be advise to steer well clear   \n",
       "\n",
       "   true_label  LR_predict  SVM_predict  SIA_predict  \n",
       "0  0           1           1            0            \n",
       "1  1           0           0            0            \n",
       "2  0           0           0            1            "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame(list(zip(test_text,test_label, LR_y_pred, SVM_y_pred, sia_y_pred_test)), columns =['test_text','true_label', 'LR_predict','SVM_predict', 'SIA_predict'])\n",
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.89      0.90     12500\n",
      "           1       0.89      0.91      0.90     12500\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     25000\n",
      "   macro avg       0.90      0.90      0.90     25000\n",
      "weighted avg       0.90      0.90      0.90     25000\n",
      "\n",
      "0.89832\n"
     ]
    }
   ],
   "source": [
    "ensemble_predict = []\n",
    "for each in zip(SVM_y_pred, LR_y_pred, value):\n",
    "    if sum(each) > 1:\n",
    "        ensemble_predict.append(1)\n",
    "    else:\n",
    "        ensemble_predict.append(0)\n",
    "\n",
    "print(classification_report(test_label, ensemble_predict))\n",
    "print(balanced_accuracy_score(test_label, ensemble_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.79      0.76     12500\n",
      "           1       0.77      0.72      0.75     12500\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     25000\n",
      "   macro avg       0.76      0.75      0.75     25000\n",
      "weighted avg       0.76      0.75      0.75     25000\n",
      "\n",
      "0.7543599999999999\n"
     ]
    }
   ],
   "source": [
    "NB_y_pred = NB_model.predict(test.toarray())\n",
    "print(\"==========Model Evalucation=========\")\n",
    "print(classification_report(test_label, NB_y_pred))\n",
    "print(\"============Test Accuracy===========\")\n",
    "print(balanced_accuracy_score(test_label, NB_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(clf, X_train, y_train=None, \n",
    "                             top_n=10, figsize=(8,8), print_table=False, title=\"Feature Importances\"):\n",
    "    __name__ = \"plot_feature_importances\"\n",
    "    \n",
    "    import pandas as pd\n",
    "    import numpy  as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    from xgboost.core     import XGBoostError\n",
    "    from lightgbm.sklearn import LightGBMError\n",
    "    \n",
    "    feat_imp = pd.DataFrame({'importance':clf.feature_importances_})    \n",
    "    feat_imp['feature'] = X_train.columns\n",
    "    feat_imp.sort_values(by='importance', ascending=False, inplace=True)\n",
    "    feat_imp = feat_imp.iloc[:top_n]\n",
    "    \n",
    "    feat_imp.sort_values(by='importance', inplace=True)\n",
    "    feat_imp = feat_imp.set_index('feature', drop=True)\n",
    "    feat_imp.plot.barh(title=title, figsize=figsize)\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.show()\n",
    "    \n",
    "    if print_table:\n",
    "        from IPython.display import display\n",
    "        print(\"Top {} features in descending order of importance\".format(top_n))\n",
    "        display(feat_imp.sort_values(by='importance', ascending=False))\n",
    "        \n",
    "    return feat_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = []\n",
    "with open('IMDB_train_skipgram.txt') as tr_ft:\n",
    "    for line in tr_ft.readlines():\n",
    "        train.append([float(each) for each in line.strip().split()])\n",
    "test = []\n",
    "with open('IMDB_test_skipgram.txt') as te_ft:\n",
    "    for line in te_ft.readlines():\n",
    "        test.append([float(each) for each in line.strip().split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'penalty': 'l1'}\n",
      "0.8586571799835462\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(C=[0.01, 0.1, 1], penalty =['l1','l2'])\n",
    "\n",
    "grid = GridSearchCV(LogisticRegression(), \n",
    "                    param_grid=param_grid, \n",
    "                    cv=5, \n",
    "                    scoring=['f1', 'roc_auc', 'balanced_accuracy'], \n",
    "                    refit='f1',\n",
    "                    n_jobs=-1)\n",
    "grid.fit(train, train_label)\n",
    "Fxt_LR_model = grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.86      0.86     12500\n",
      "           1       0.86      0.86      0.86     12500\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     25000\n",
      "   macro avg       0.86      0.86      0.86     25000\n",
      "weighted avg       0.86      0.86      0.86     25000\n",
      "\n",
      "0.8624\n"
     ]
    }
   ],
   "source": [
    "Fxt_LR_y_pred = Fxt_LR_model.predict(test)\n",
    "print(classification_report(test_label, Fxt_LR_y_pred))\n",
    "print(balanced_accuracy_score(test_label, Fxt_LR_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 4, 'gamma': 1, 'kernel': 'rbf'}\n",
      "0.8671051176527236\n"
     ]
    }
   ],
   "source": [
    "param_grid = dict(gamma=[1], C=[1, 4], kernel=[ 'linear'])\n",
    "\n",
    "grid = GridSearchCV(SVC(), \n",
    "                    param_grid=param_grid, \n",
    "                    scoring=['f1'], \n",
    "                    refit='f1',\n",
    "                    n_jobs=-1)\n",
    "grid.fit(train, train_label)\n",
    "Fxt_SVM_model = grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Fxt_SVM_y_pred = Fxt_SVM_model.predict(test)\n",
    "print(classification_report(test_label, Fxt_SVM_y_pred))\n",
    "print(balanced_accuracy_score(test_label, Fxt_SVM_y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
